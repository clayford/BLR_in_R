---
title: "Binary Logistic Regression in R"
author: "Clay Ford, UVA Library"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
plot(cars)
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## CODE ALONG 0

Enter a new code chunk and run the code `rbinom(n = 10, size = 1, prob = 0.5)`. This generates random values from Binomial Distribution with trial size = 1 and probability = 0.5. (this is like flipping a fair coin 10 times)



## Load packages

We'll use the following packages in this workshop.

```{r}
library(vcdExtra)
library(car)
library(effects)
library(ggeffects)
library(emmeans)
```


## Proportions and Probability

Pick a UVA student at random. What is the probability they have at least one tattoo? This is a _binary_ response. Only two possible answers: yes or no. We don't know, so we take a random sample. Let's say we randomly sample 100 students and find that 15 students have tattoos. 

The _proportion_, 15/100 = 0.15, could be interpreted as an estimated _probability_ that a UVA student has a tattoo. Recall that proportions and probabilities range from 0 to 1. We might conclude about 15% of UVA students have tattoos, or there's a probability of about 0.15 that a randomly selected UVA student has a tattoo.

What if we sampled 100 males and 100 females and compared the proportion with tattoos? Perhaps 0.18 of males have tattoos and 0.12 of females have tattoos. This suggests the probability of having a tattoo may be related to sex.

What if we also collected other information such as cumulative GPA or Greek life (yes or no)? Again this suggests the probability of having a tattoo may be related to multiple _variables_ or _predictors_.

_Binary Logistic Regression_ is a method that allows us to investigate questions such as this. It allows us to _model the probability_ of a binary event given multiple predictors.

Statistical modeling boils down to 3 steps:

1. propose and fit a model
2. assess if model is good
3. use model to make predictions or quantify relationships

## Load Data for the workshop

Today we'll work with data on Intensive Care Unit (ICU) admissions. This data are a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult ICU, derived from Hosmer, Lemeshow and Sturdivant (2013) and included with the vcdExtra package, Friendly (2022).

Below we read in the data from GitHub where it is stored as an _rds_ file. The rds format is an efficient way to store R objects. To read in an rds file we use the `readRDS()` function. To read in from a web site, we need to use the `url()` function to open a connection to the site.

```{r}
URL <- "https://github.com/clayford/BLR_in_R/raw/main/data/icu.rds"
icu <- readRDS(file = url(URL))
names(icu)
```

### List of variables

- died: Did patient die in ICU? (No or Yes)
- age: Patient age
- sex: Patient sex (Female or Male)
- cancer: Cancer part of present problem? (No or Yes)
- systolic: Systolic blood pressure at admission (mm Hg)
- admit: Type of admission (Elective or Emergency)
- ph: pH from initial blood gases (>=7.25 or <7.25)
- pco: partial pressure of carbon dioxide (PCO2) (<=45 or >45)
- uncons: patient unconscious at admission? (No or Yes)

The goal today is to teach the basics of Binary Logistic Regression by developing a model to predict the probability of death after admission to this ICU and to study the risk factors associated with ICU mortality. 


## Explore the ICU Data

The response variable is "died". Let's count the responses using the `xtabs()` function. The argument `addNA = TRUE` reports missing values if any are present. `xtabs()` uses formula notation. `~ died` means count the unique values of `died`.

```{r}
xtabs(~ died, data = icu, addNA = TRUE)
```

We can get the proportion of each category by using the `proportions()` function. Notice we can "pipe" the result of `xtabs()` into `proportions()` using the base R pipe operator, `|>`. (The base R pipe operator was introduced in R version 4.1.)

```{r}
xtabs(~ died, data = icu) |> proportions()
```

Just based on this information, a naive estimate of the probability of death (Yes) after admission to the ICU is about 0.20. But this probability estimate may change based on other predictors such as patient age, sex, whether or not patient was conscious at ICU admission, etc.

How does being unconscious at time of admission to ICU affect proportion of deaths? This time we use `xtabs()` to create a cross tabulation, and then pipe into `proportions()` with `margin = 1` to specify we want to calculate proportions for the rows. In other words, find proportion of `died` conditional on `uncons`. Of those who were unconscious at admission about 87% died. But only 15 out of 200 people were unconscious at admission.

```{r}
xtabs(~ uncons + died, data = icu) |>
  proportions(margin = 1) |>
  round(3)
```

How does age relate to the proportion of died? Plotting a factor as a function of a numeric variable using `plot()` creates a _spineplot_. The x-axis groups age into categories. Each vertical box shows the proportion of died within each age category. The width of the box corresponds to the proportion of age categories. Age 60 - 70 appears to be the largest group. It appears the proportion of deaths creeps up with increasing age.

```{r}
plot(died ~ age, data = icu)
```

## CODE ALONG 1

- create a cross tabulation of admit and died
- Plot `died` as a function of `systolic`.

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).



## Binary Logistic Regression basics

Binary logistic regression produces a _model_ that we can use to make probability predictions or quantify relationships. For example, with the ICU data we might...

- predict probability of death given age, sex, and whether you were unconscious at admission
- determine how much the probability of dying increases (or decreases) if you were unconscious at admission

_We are responsible for proposing and assessing the model._ For example, the "sex" variable may or may not help our model. We have to decide to include it or not. Subject matter expertise is just as important as statistical expertise.

One way to fit binary logistic regression models in R is with the `glm()` function. GLM = Generalized Linear Model.

Let's fit a model. Notice we must set `family = binomial` because our response is binary. We almost always want to save the model result. The formula says "model `died` as a function of `age`, `sex`, and `uncons`." Call `summary()` on the model object to see the result.

```{r}
m <- glm(died ~ age + sex + uncons, data = icu, family = binomial)
summary(m)
```

The model is in the _Estimate_ column of the coefficients section. Here's the model expressed in basic math notation, with values rounded to two decimal places:

$$\text{died} = -3.58 + 0.03\text{ age} + 0.14\text{ sex} + 3.60\text{ uncons}$$

If we plug in values for `age`, `sex`, and `uncons` we get _predicted log odds_ of dying, not probability. 

Log odds are also known as _logit_ values. The logit transformation takes probability, which ranges from [0,1], and expresses it as log odds, which ranges from [-Inf,+Inf]. This is desirable since modeling probability directly could produce predictions below 0 or above 1.

Let's use our model to make a prediction for a male age 70 who was unconscious at admission. We use the `predict()` function with a data frame containing our predictors.

```{r}
predict(m, newdata = data.frame(age = 70, sex = "Male", uncons = "Yes"))
```

This obtained by plugging those values into our model:

$$2.145 = -3.58 + 0.03(70) + 0.14(1) + 3.60(1)$$

The prediction, 2.145, is hard to interpret. To get probability we need to take
the _inverse logit_, which converts log odds back to the probability scale. Fortunately we can just set `type = "response"` in the `predict()` function.

```{r}
predict(m, newdata = data.frame(age = 70, sex = "Male", uncons = "Yes"), 
        type = "response")
```

How did we get that probability? The formula for the inverse logit is as follows:

$$\text{Probability} =  \frac{e^x}{1 + e^x}$$

We can verify this is how we converted log-odds to probability.

```{r}
exp(2.145336)/(1 + exp(2.145336))
```

We predict a very high probability, about 0.90, of dying given you're a 70 year old male who was unconscious at admission to the ICU.

Again we should assess this model and think carefully before we fall in love with our prediction! We'll do this shortly.


## Probability versus Odds versus Log Odds

Let p be probability.    
Odds = p/(1-p).     
Log odds is log(odds).   
 
Let's convert a few probabilities to odds and log odds. The function `fractions()` from the MASS package returns fractions, which is usually how odds are expressed. (We use `as.character()` to print the fraction bar.) Notice a few things:

- odds are always positive
- log odds can be positive or negative
- small changes in probability can result in "big" changes in log odds. log odds change rapidly when p is close to 0 or 1. In general, never a good idea to round log odds. 

```{r}
p <- c(0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 0.999)
odds <- MASS::fractions(p/(1-p))
logodds <- log(odds)
data.frame(p, odds = as.character(odds), logodds)
```

No intro to logistic regression would be complete without a plot of probability versus log odds. Notice that no matter how big or small the log odds, the corresponding probability is always between 0 and 1.

```{r}
p <- seq(0.0001, 0.9999, length.out = 1000)
logodds <- log(p/(1-p))
plot(logodds, p, type = "l")
```


R has built-in functions for converting probability to log odds and log odds to probability. 

- `plogis`: convert log odds to probability
- `qlogis`: convert probability to log odds

Convert log odds of 2 to probability.

```{r}
plogis(2)
```

Convert probability of 0.8807971 to log odds.

```{r}
qlogis(0.8807971)
```

You probably don't need these functions when doing logistic regression in R, but they can be useful to know. 

## How to interpret coefficients

Our model is comprised of simple additive predictors (ie, _main effects_). Therefore the coefficients can be interpreted. We can use the `coef()` function to extract the coefficients.

```{r}
coef(m)
```

These coefficients are on the log odds scale. For example, the age coefficient says the log odds of ICU death increase by about 0.028 for every one year increase in age, holding all other variables constant. 

If we _exponentiate the coefficients_ we get _odds ratios_. An odds ratio is a ratio of two odds. For example if we exponentiate the age coefficient, we get 1.028719. 

```{r}
exp(coef(m)["age"])
```

This says the odds of ICU death increase by about 1.029 for each one year increase in age. Put another way, the odds of dying when you're 70 are about 3% higher than the odds of dying when you're 69, holding all other variables constant. (Recall that multiplying by 1.03 is the same as increasing by 3%.)

Let's do an odds ratio "by hand".

Say an event has a probability of 0.8 of happening. The corresponding odds are 4, or 4 to 1. We can expect 4 "successes" for every one "failure". 

```{r}
# event 1
p1 <- 0.8
odds1 <- p1/(1-p1)
odds1
```

Say another event has a probability of 0.6 of happening. The corresponding odds are 1.5.

```{r}
# event 2
p2 <- 0.6
odds2 <- p2/(1-p2)
odds2  # 3/2
```

The odds ratio of the two events is 4/1.5, which is about 2.7. 

```{r}
odds1/odds2
```

This says the odds of event 1 are about 2.7 times higher than the odds of event 2.

An odds ratio of 1 implies the odds are the same, and hence there is no difference between the event probabilities. They are equally likely.

Let's look at the odds ratios of our model:

```{r}
exp(coef(m))
```

Our model claims the odds of dying when unconscious are about 36 times higher than the odds of dying when you are not unconscious! We can calculate this "by hand" by predicting probabilities for, say, two Males aged 60, one who was unconscious and the other who was not:

```{r}
# unconscious male, age 60
p1 <- predict(m, newdata = data.frame(age = 60, sex = "Male", 
                                      uncons = "Yes"),
              type = "response")
# conscious male, age 60
p2 <- predict(m, newdata = data.frame(age = 60, sex = "Male", 
                                      uncons = "No"),
              type = "response")
# odds ratio
odds_p1 <- p1/(1-p1)
odds_p2 <- p2/(1-p2)
or <- odds_p1/odds_p2
cbind(p1, p2, `odds ratio` = or)
```

The odds ratio for age is about 1.03. This says the odds of dying increase about 3% for every 1 year increase in age. If we wanted to estimate that for every 10 years increase in age, we simply multiply the coefficient by 10.

```{r}
exp(coef(m)["age"] * 10)
```

Every 10 year increase in age increases the odds of dying by about 33%, all else held constant.

## Standard errors and hypothesis tests

If we use `coef()` on `summary()` we extract the coefficient table which contains standard errors, test statistics (z value), and p-values.

```{r}
coef(summary(m)) 
```

- Std. Error: quantifies uncertainty of estimated coefficient. It may help to imagine adding and subtracting it to your estimate to get an idea of the variability of your estimate.
- z value: Ratio of Estimate to Std. Error. Large ratios mean coefficients with relatively small standard errors. z values larger than 2 or 3 are usually considered "big".
- Pr(>|z|): probability of getting a value as big or bigger than |z| if coefficient is 0, given other predictors already in model.

_Altogether this information is used to assess whether predictor coefficients are reliably positive or negative (ie, different from 0)_

P-values only assess whether a coefficient is different from 0. It's better to assess _direction and magnitude_ of coefficients with confidence intervals. Use `confint()` on the model object.

```{r}
confint(m)
```

We're not sure if being Male increases or decreases the chance of dying. The lower bound is negative and the upper bound is positive (though not by much). Probably worth noting that there 124 male subjects in this data set versus only 76 female subjects.

Using `exp()` with `confint()` provides a _confidence interval on the odds ratios_. Of interest here is whether or not the CI crosses 1. A ratio of 1 implies no difference in odds.

```{r}
exp(confint(m))
```

The effect of age seems small, at least per one year increase. The effect of being unconscious seems highly variable. That's because we only have 15 subjects who arrived unconscious.

## CODE ALONG 2

- Model died as a function of age, admit, and uncons. Save the model as `m2`.
- Use `summary()` and `confint()` on the model
- Interpret the coefficient for admit as an odds ratio
- What's the predicted probability of dying for a person age 50 with Emergency admission and unconscious?

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).



## Confidence intervals on predictions

A prediction is just a point estimate. There is some uncertainty in the estimate. A confidence interval helps quantify the uncertainty. Unfortunately the `predict()` function for binary logistic regression models does not provide a confidence interval for predictions. However we can calculate it ourselves.

Earlier we made a prediction for a male, age 60 who was admitted unconscious to the ICU as follows:

```{r}
predict(m, newdata = data.frame(age = 60, sex = "Male", uncons = "Yes"),
              type = "response")
```

While there is no option for a confidence interval, we can request the standard error of the prediction by specifying `se.fit = TRUE`. Notice below we request the predicted log odds instead of the response. This is so we get a standard error on the log odds scale. This will prevent us from having  confidence interval that goes below 0 or above 1. We save to object "pred" for convenience.

```{r}
pred <- predict(m, newdata = data.frame(age = 60, sex = "Male", 
                                        uncons = "Yes"),
                se.fit = TRUE)
pred
```

Now we add and subtract 1.96 standard errors to the prediction to get a 95% confidence interval on the log odds scale, and then use the inverse logit to transform to probabilities.

```{r}
plogis(pred$fit + c(-1,1)*1.96*pred$se.fit)
```

We can also use the `ggpredict()` function from the {ggeffects} package to calculate that confidence interval. Instead of supplying new data in data frame we use the `terms` and `condition` arguments. The `terms` are the variables of interest. The `condition` are the variables we want to condition on. Below we arbitrarily set age as the term and sex and uncons as the conditions. `age[60]` says set age to 60. `condition = c(sex = "Male", uncons = "Yes")` says to hold those two variables at "Male" and "Yes", respectively.

```{r}
ggpredict(m, terms = "age[60]", 
          condition = c(sex = "Male", uncons = "Yes"))
```

The term could also be sex with age and uncons defined in the condition.

```{r}
ggpredict(m, terms = "sex[Male]", 
          condition = c(age = 60, uncons = "Yes"))
```

This is admittedly a little weird. `ggpredict()` is usually employed to generate many predictions for a term of interest. For example:

```{r}
ggpredict(m, terms = "age[30:60 by=5]", 
          condition = c(sex = "Male", uncons = "Yes"))
```


Below we show how to visualize binary logistic regression models using the {ggeffects} and {effects} packages, both of which calculate confidence intervals for predictions using the same method we just demonstrated. 

## Model assessment

It's easy to use `glm()` to fit a binary logistic model, but is the model "good"? Is one model better than another? How do we know which predictors to keep and which ones to drop? Should we allow interactions or non-linear transformations? _Model building is hard_ and requires equal parts statistical expertise and subject matter expertise. 

### Is our model better than the NULL model?

One question to consider: _is our model better than the NULL model_, where the NULL model is simply the proportion of "successes". Recall 20% of the patients admitted to the ICU died.

```{r}
# proportion of died = "Yes"
mean(icu$died == "Yes")
```

The NULL model is simply predicting anyone admitted to the ICU has a 0.20 probability of dying. This is sometimes called the _intercept-only model_, which we can fit as follows:

```{r}
# died ~ 1 is the intercept-only model
m0 <- glm(died ~ 1, data = icu, family = binomial)
coef(m0)
```

The intercept coefficient is the predicted _log odds_ of dying. We can convert log odds to probability using the inverse logit, which is available as the `plogis()` function. This turns out to be a fancy way to get 0.2.

```{r}
# convert log odds to probability
plogis(coef(m0))
```

We can formally assess if a proposed model is better than the NULL model using the base R `anova()` function. Recall our model `m`

```{r}
formula(m)
```

How much better is it than the NULL model? Call `anova()` with `test = "Chisq"`:

```{r}
anova(m, test = "Chisq")
```

Notice that terms are added sequentially, _in order_. This is sometimes called a _Type I test_.

- A model with just age appears to be better than the NULL model. 
- Adding sex with age already in the model doesn't seem to make any further improvement. 
- Adding uncons with age and sex already in the model further improves the model quite a bit. Notice the drop in _Residual Deviance_. 

Deviance is a _statistical summary of model fit_. A "good" predictor should drop Residual Deviance by more than 1. Notice sex produces no change in deviance, but uncons drops it more than 30.

See this article for an explanation of what exactly deviance residuals are:
https://library.virginia.edu/data/articles/understanding-deviance-residuals

An alternative to Type I tests are _Type II tests_, which test the contribution of each variable assuming all others are included. The base R `drop1()` function produces Type II tests. This technically does not compare the model to the NULL model. However it does allow us to asses each variable's contribution to the model.

```{r}
drop1(m, test = "Chisq")
```

The substance of the results are largely the same, however each test assumes the other variables are in the model. For example, adding age to a model with sex and uncons already present appears to improve the model.

### Is our model worse than the SATURATED model?

A second question to consider: _how bad is our model compared to a saturated model that fits the data perfectly_? The `LRstats()` function from the {vcdExtra} package helps us to assess this question. It conducts a hypothesis test with the Null being "the model fit is no different from the saturated model". A low p-value provides evidence against this hypothesis.

```{r}
LRstats(m)
```

The result of this test fails to provide evidence against the Null. That's good.

See also the "Assessing predictive accuracy" section at the end for more information. 

## CODE ALONG 3

Assess model `m2` using the methods we just described.

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).



## Fitting more complex models

So far our models only contain _main effects_. The effects are _additive_. It doesn't matter what age you are, the effect of uncons is the same. The effect of age is also constant. The effect of age going from 20 to 21 is the same as the age going from 75 to 76.

We can relax that assumption by allowing variables to _interact_ and to allow variables to have _non-linear_ effects. These make our models more complex and more difficult to interpret. But they also may produce better models that more accurately explain the probability of our response.

Note: more complex models require more data. See the _Minimum sample sizes_ section below.

### Interactions

When variables interact, that means we cannot untangle their effects. The effect of one depends on another. For example, perhaps the effect of age depends on uncons status. 

Below we allow age and uncons to interact by adding `age:uncons` to the model. That does not mean they really interact. We're simply _allowing them to interact in our model_. The evidence for the hypothesized interaction seems rather weak according to the small z value and relatively high p-value.

```{r}
m_int <- glm(died ~ age + sex + uncons + age:uncons, data = icu,
             family = binomial)
summary(m_int)
```

### Non-linear effects

A non-linear effect is not constant. The slope or trajectory changes over the range of values. For example, humans don't grow in height at a constant rate. We don't grow 3 inches per year until we die. We tend to grow at varying rates until we stop in our late teens or 20s.

Below we allow age to have a non-linear effect by using a _natural spline_ with 2 degrees of freedom. This basically allows the effect of age to change trajectory twice if needed. It's somewhat similar to polynomial regression, but better behaved in the extremes of the data. The splines package comes with R. The `ns()` function can be used directly in the model formula. The second argument specifies how many times you want to let the effect "bend", or change direction. 

```{r}
library(splines)
m_nle <- glm(died ~ ns(age, 2) + sex + uncons, data = icu,
             family = binomial)
summary(m_nle)
```

It's hard to tell if the spline is necessary based on the summary output. One proper way to assess this is using the `anova()` function, where we can compare model with a linear effect age to the model with a nonlinear effect of age. This tests whether or not we need the non-linear term. The null hypothesis of this test is that both models are equally good, which implies we don't need the non-linear effect for age.

```{r}
anova(m, m_nle, test = "Chisq")
```

It appears we don't benefit from the non-linear effect.


## Visualizing models

Adding interactions and/or non-linear terms make model interpretation extremely difficult. One way to make sense of your model is to use _effect displays_, or _effect plots_. 

The basic idea is to pick a variable of interest, say age, and make predictions for it over a range of values while holding other predictors constant.

Below we show two ways to do this using the effects and ggeffects packages. 

First visualize `m_nle` using `Effect()` from the {effects} package. Notice we can pipe the result into `plot()`. The light blue ribbon is a 95% confidence band. The little ticks at the bottom are the observed values. Notice the ribbon gets larger (more uncertain) when we have fewer observations, which we see in the extremes of the data. It appears the effect of age doesn't change until after about age 55, and then it begins to increase. Also notice the y-axis is on the log-odds scale but labeled with probabilities. (set `type = "response"` to scale and label the y-axis to probabilities.)

```{r}
Effect("age", mod = m_nle) |> 
  plot()
```

We can make a {ggplot2} version of this plot using `ggeffect()` from the ggeffects package. Again we can pipe the result into `plot()`. Set `log.y = TRUE` to get the same re-scaled y-axis as the plot from the {effects} package. Set to FALSE to rescale and label y-axis on the probability scale.

```{r}
ggeffect(model = m_nle, ~ age) |> 
  plot(log.y = TRUE)
```

Next, we visualize m_int which models an interaction between age and uncons. This allows the effect of uncons to depend on age, and vice versa. (Recall the interaction was small and the p-value was about 0.11.) Using `Effect()`, we specify both variables as the focal predictors. The first variable will be plotted on the x-axis.

```{r}
Effect(focal.predictors = c("age", "uncons"), mod = m_int) |>
  plot(type = "response")
```

There is a great deal of uncertainty about the effect of age when a patient is unconscious. That's because of the 200 patients, only 15 were admitted unconscious. And no one under the age of 40 was unconscious when admitted to the ICU.

We create the same plot using `ggeffect()` with a vector of predictors as well. 

```{r}
ggeffect(model = m_int, terms = ~ age + uncons) |>
  plot(facets = TRUE)
```


## CODE ALONG 4

- Fit a new model with the following formula and save as `m3`. (Systolic is the systolic blood pressure at admission): 
  `died ~ age + admit + uncons + age:uncons`   
- Does the interaction seem necessary?
- Visualize the interaction.

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).


## Comparing models

We can compare models by using criteria such as AIC and BIC as well as hypothesis tests. Let's review both.

AIC and BIC are statistics for measuring goodness of fit. In isolation these values are not meaningful, but when compared for different models with the same response, _lower values are better_. The `LRstats` function shows both criteria. Let's compare models `m` and `m2`:

```{r}
LRstats(m, m2)
```

Even though both models appear to be just as good as a saturated model, `m2` appears to better than `m`.

When one model is _nested_ in another, we can use hypothesis tests to assess whether the simpler model is just as good as the more complex model. Let's fit two models, one nested in the other.

Model 1: age and uncons  
Model 2: age, uncons and their interaction

Model 2 says the effect of age depends on uncons and vice versa. Model 1 is nested within Model 2 since it's a special case of Model 2 with an interaction coefficient of 0.

```{r}
m4 <- glm(died ~ age + uncons, data = icu, family = binomial)
m5 <- glm(died ~ age + uncons + age:uncons, data = icu, family = binomial)
```

Now we can carry out a formal test to compare the models using the `anova()` function. The null hypothesis is both models fit equally well. A low p-value provides evidence against the null.

```{r}
anova(m4, m5, test = "Chisq")
```

The interaction may be useful but is probably small. Whether or not to keep it is just as much a subject matter question as it is a statistical one. Comparing AIC and BIC values is also inconclusive.

```{r}
LRstats(m4, m5)
```

## Influence and Diagnostic Plots

One or two influential observations can completely change a model. For example, a notable interaction may be due to an observation's unusual values. 

An effective plot for quickly assessing influence is the `influencePlot()` function from {car} package. It plots three difference diagnostics:

1. _residuals_ on y axis: how different is an observed response from its predicted response
2. _Hat values_ on x axis: measures "leverage", potential impact of a case on the model
3. _Cook's distance_ as size of points: influence of observation on regression coefficients

Let's use this plot with our `m3` model.

```{r}
m3 <- glm(died ~ age + admit + uncons + age:uncons,
          data = icu, family = binomial)
```

Assigning the result creates a data frame of the most influential values.

```{r}
infl <- influencePlot(m3)
```

Here are the exact values of the labeled points. Nothing is too outrageous. A Cook's D bigger than 1 is the usual red flag. A Hat Value is considered large if it exceeds (2 or 3)(k/n) where k is the number of model coefficients and n is number of observations. Here that comes out to (2 or 3)(7/200) or about (0.07 - 0.11). Residuals larger than 2.5 may be of interest.

```{r}
infl
```

We can extract the rownames and use them to subset the icu data frame to see the observed values. 

```{r}
pred <- predict(m3, type = "response")[rownames(infl)]
obs <- icu[rownames(infl), c("died", "age", "cancer", 
                             "admit", "uncons", "systolic")]
cbind(obs, pred)
```

- Subject 84: model predicts death at 96%, but subject lived
- Subject 881: one of the older subjects at age 89 and lived
- Subject 202: model predicts death at 4%, but subject died
- Subject 208: subject had Elective admit but arrived unconscious? (typo?)

One action to take based on these results is a _sensitivity analysis_ where we fit the model without one or more of the data points to see how the model changes. Here's one way to fit a new model that does not include the influential points identified by `influencePlot`, using `subset = !rownames(icu) %in% rownames(infl)`. Then we compare coefficients from both models using the `compareCoefs()` function from the {car} package.

```{r}
m3a <- glm(died ~ age + admit + uncons + age:uncons,
          data = icu, family = binomial, 
          subset = !rownames(icu) %in% rownames(infl))
compareCoefs(m3, m3a)
```

Removing the four influential observations absolutely blows up the standard errors! That's because we now have cells with 0 counts.

```{r}
xtabs(~ died + uncons + admit, data = icu, 
      subset = !rownames(icu) %in% rownames(infl))
```

Be careful about reflexively dropping observations, "influential" or otherwise.

## CODE ALONG 5

- Fit a new model with formula `died ~ systolic + admit + uncons`. Call the model `m6`.
- Check the model influential observations using the `influencePlot()` function. Save the result as `infl_m6`.
_ Compare the model to `m2`.

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).


## We're done!

See the appendix for more material cut for time.

For help with statistical analysis, contact the UVA Library StatLab: `statlab@virginia.edu`

## References

- Friendly M, Meyer D (2016). Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data. CRC Press.
- Friendly M (2022). _vcdExtra: 'vcd' Extensions and Additions_. R package version 0.8-0, <https://CRAN.R-project.org/package=vcdExtra>.
- Lemeshow, S., Teres, D., Avrunin, J. S., Pastides, H. (1988). Predicting the Outcome of Intensive Care Unit Patients. Journal of the American Statistical Association, 83, 348-356.
- John Fox (2003). Effect Displays in R for Generalised Linear Models. Journal of Statistical Software, 8(15), 1-27. doi 10.18637/jss.v008.i15
- Lüdecke D (2018). "ggeffects: Tidy Data Frames of Marginal Effects from Regression Models." _Journal of Open Source Software_, *3*(26), 772. doi:10.21105/joss.00772  <https://doi.org/10.21105/joss.00772>.
- R Core Team (2022). R: A language and environment for statistical  computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
- Harrell F (2015). Regression Modeling Strategies, 2nd ed. Springer. http://hbiostat.org/rmsc/


## Appendix


### Three ways to fit a binary logistic regression model

The `glm()` function allows binary response data to be structured in three different ways:

1. As a 0/1 indicator or factor. This is what we presented in this workshop
2. As a proportion with the total number of cases given by the `weights` argument.
3. As a two-column matrix, with the first column giving the number of successes and the second the number of failures.

Let's demonstrate all three ways using a subset of the icu data.

#### 1. As a 0/1 indicator

Let's say we want to model died as function of uncons and age. Our data is already structured as one row per event with a 0/1 indicator.

```{r}
mod1 <- glm(died ~ uncons + age, data = icu, family = binomial)
summary(mod1)
```

#### 2. As a proportion

To demonstrate this we need to first restructure the data. To begin, we need to set the response as numeric 0 and 1. The died variable is currently a factor.

```{r}
icu$died <- ifelse(icu$died == "Yes", 1, 0)
```

Next we get the total deaths for each uncons and age combination. For example, we have 3 deaths for subjects age 75 who were not unconscious upon submission to the icu.

```{r}
# sum of died by uncons and age
x <- aggregate(died ~ uncons + age, data = icu, sum)
x[x$age %in% 72:75,]
```

Next we get the total number of observations for each age and uncons combination. For example, we have 2 observations for subjects age 16 who came to the icu conscious.

```{r}
# sum of observations by uncons and age
n <- aggregate(died ~ uncons + age, data = icu, length)
head(n)
```

Finally we combine the data into a data frame. We name the number of observations for each uncons/age combination as "trials" and the proportion of deaths per combination as "proportion".

```{r}
# combine into data frame and create column for proportions
icu2 <- data.frame(uncons = x$uncons, 
                   age = x$age, 
                   died = x$died, 
                   trials = n$died,
                   proportion = x$died/n$died)
head(icu2)
```

Now we use "proportion" as the response, but also include the argument `weights = trials` to indicate total number of observations at each uncons and age combination.

```{r}
mod2 <- glm(proportion ~ uncons + age, data = icu2, family = binomial,
            weights = trials)
summary(mod2)
```

Notice the coefficient estimates are identical but the deviance statistics are different.

We could also use trials and died on-the-fly by entering `died/trials` as the response, like so:

```{r}
mod2 <- glm(died/trials ~ uncons + age, data = icu2, family = binomial,
            weights = trials)
```


#### 3. As a two-column matrix

In this scenario, the response is a two-column matrix, with the first column giving the number of deaths and the second the number of survivors. We need to add number of survivors to our data frame, which is simply the difference between trials and died.

```{r}
icu2$survivor <- icu2$trials - icu2$died
```

Now use `cbind()` to create a matrix of "died" and "survivor" and use that as our response. We no longer need to use the weights argument in this case.

```{r}
mod3 <- glm(cbind(died, survivor) ~ uncons + age, 
            family = binomial, data = icu2)
summary(mod3)
```

The result for the third method is identical to the result for the second method.

_Why one method versus the other?_

Even though all three methods produce the same model coefficients, method 1 produces residuals that differ from methods 2 and 3.

Method 1 produces a residual for each unique predictor value. The resulting residual versus fitted plot produces weird patterns that make it hard to assess model fit. Below is a plot for `mod1`. In this case a residual is `1 - predicted probability` or `0 - predicted probability` depending on the observed response. 

```{r}
plot(mod1, which = 1)
```

Compare to a residual versus fitted plot for `mod2` and `mod3`. This looks similar to the types of residual versus fitted plots we get for linear models. 

```{r}
plot(mod2, which = 1)
```

rows 39 and 74 of icu2 reveal two older patients who came to icu unconscious but lived. The model predicts a high probability of them dying, but they did not. Hence the low, negative position in the plot. We observe them living but the model predicts high probability of death. The residual is therefore negative. The plot suggests these subjects had a lower probability of death than the model predicts. 

```{r}
icu2$fitted <- fitted(mod2)
icu2[c(39,74),]
```


### Simulating Binary Data

It can be useful to know how to simulate binary data. This can aid in preparing for a study or making power and sample size estimates.

Pretend we want to simulate data for the binary event of a UVA student having a tattoo with probability associated with age and whether or not parent/guardian has tattoo. First generate age and pt (parent has tattoo) variables. Assume a sample size of 200. We'll center the age variable so 0 is the mean.

```{r}
n <- 200
set.seed(1)
age <- sample(18:30, size = n, replace = TRUE)
age_c <- age - mean(age) # center age so mean is 0.
pt <- sample(c("no", "yes"), size = n, replace = TRUE)
```

Now use those variables to simulate a response on the _log odds scale_. Let's say each one year increase in age increases the odds of having a tattoo by about 10% and that having a parent with a tattoo increases the odds of having a tattoo by about 65%. This implies coefficients of 0.1 and 0.5, respectively, which I determined through some trial and error. We'll also assume an intercept of -1.73, which implies a probability of 0.15 for having a tattoo if you're the mean age and your parents had no tattoo.

```{r}
exp(0.1) # about 10% increase in odds
exp(0.5) # about 65% increase in odds
qlogis(0.15) # prob of tattoo at mean age and parents w/out tattoo
```

Now calculate tattoo_log_odds as follows.

```{r}
tatoo_log_odds <- -1.73 + 0.1*age_c + 0.5*(pt == "yes")
```

Next convert the log odds to probability using `plogis`, which returns the inverse logit, or inverse log odds.

```{r}
tattoo_prob <- plogis(tatoo_log_odds)
```

Notice our probabilities range from  about 0.09 to 0.35.

```{r}
summary(tattoo_prob)
```

Now we can use those probabilities to return a 0 or 1 using the `rbinom()` function. This simulates data from a _binomial distribution_. We will use each individual probability to basically flip a coin with the stated probability and get a 0 or 1.

```{r}
set.seed(2)
tattoo <- rbinom(n = n, size = 1, prob = tattoo_prob)
```

Let's tally up our simulated events.

```{r}
xtabs(~ tattoo)
```

A student with a tattoo seems more likely to have a parent with a tattoo. That's according to our simulation specifications.

```{r}
xtabs( ~ tattoo + pt)
```

Likelihood of having a tattoo also increases with age. Again this is per our specification.

```{r}
plot(factor(tattoo) ~ age)
```


Now let's use `glm()` to work backwards and see how close we get to recovering the true coefficients we used to simulate the data.

```{r}
mod <- glm(tattoo ~ age_c + pt, family = binomial)
# True model: -1.73 + 0.1*age_c + 0.5*(pt == "yes")
summary(mod)
```

Pretty close, though there is a lot of uncertainty about the pt variable.

Let's pretend this model is the TRUTH about the population. Is 200 subjects a large enough sample to reliably detect the positive effect of a parent having a tattoo on the probability of a student having a tattoo?

We can simulate data and fit a model many times and then see the proportion of times we get a p-value on the pt variable that is below, say, 0.05. For example the following code returns TRUE/FALSE:

```{r}
coef(summary(mod))["ptyes", "Pr(>|z|)"] < 0.05
```

We can use the `replicate()` function with this code along with our code above to sample and fit many models and see if we detect the "pt" effect. Simply copy and paste the relevant portions of code from above into the {} for the `expr` argument. Use the `n` argument to specify the number of replications. 

```{r}
rep.out <- replicate(n = 500, expr = {
  n <- 200
  age <- sample(18:30, size = n, replace = TRUE)
  age_c <- age - mean(age) 
  pt <- sample(c("no", "yes"), size = n, replace = TRUE)
  tatoo_log_odds <- -1.73 + 0.1*age_c + 0.5*(pt == "yes")
  tattoo_prob <- plogis(tatoo_log_odds)
  tattoo <- rbinom(n = n, size = 1, prob = tattoo_prob)
  mod <- glm(tattoo ~ age_c + pt, family = binomial)
  coef(summary(mod))["ptyes", "Pr(>|z|)"] < 0.05
})
```

Now take the mean of rep.out, which is a vector of TRUE/FALSE values. This returns the proportion of times we reject the null of no "pt" effect. We might use this as an estimate of power.

```{r}
mean(rep.out)
```

It appears this "experiment" is not very powerful to detect the parent effect on having a tattoo, assuming our hypothesized coefficients are true. What if we increase the sample size to 1000?

```{r}
rep.out <- replicate(n = 500, expr = {
  n <- 1000
  age <- sample(18:30, size = n, replace = TRUE)
  age_c <- age - mean(age)
  pt <- sample(c("no", "yes"), size = n, replace = TRUE)
  tatoo_log_odds <- -1.73 + 0.1*age_c + 0.5*(pt == "yes")
  tattoo_prob <- plogis(tatoo_log_odds)
  tattoo <- rbinom(n = n, size = 1, prob = tattoo_prob)
  mod <- glm(tattoo ~ age_c + pt, family = binomial)
  coef(summary(mod))["ptyes", "Pr(>|z|)"] < 0.05
})
mean(rep.out)
```

This seems much better. But this assumes we have the resources to sample 1000 students.




### Minimum sample sizes

Harrell (2015) shows the minimum sample size required to estimate an intercept-only logistic regression model to within 0.1 of the true probability with 95% confidence is _96_, assuming true probability is 0.5.

We can show this using simulation. Below we simulate 50 observations using p = 0.5. We then fit an intercept only model and extract the estimated probability.

```{r}
y <- rbinom(50, size = 1, prob = 0.5)
m <- glm(y ~ 1, family = binomial)
plogis(coef(m))
```

Let's replicate this 1000 times and see how often we come within 0.1 of the true probability, 0.5. We simply copy-and-paste our code above in between `{}` and use that for the `expr` argument in the `replicate()` function. When I ran this I calculated about 0.85.

```{r}
r <- replicate(n = 1000, expr = {
  y <- rbinom(50, size = 1, prob = 0.5)
  m <- glm(y ~ 1, family = binomial)
  abs(plogis(coef(m)) - 0.5) < 0.1}
  )
mean(r)
```

Let's try n = 70. This returned about 0.90 for me.

```{r}
r <- replicate(n = 1000, expr = {
  y <- rbinom(70, size = 1, prob = 0.5)
  m <- glm(y ~ 1, family = binomial)
  abs(plogis(coef(m)) - 0.5) < 0.1}
  )
mean(r)
```

Finally let's try n = 100. This returned 0.952 for me, expressing that we came within 0.1 of the true value (0.5) about 95% of the time. 

```{r}
r <- replicate(n = 1000, expr = {
  y <- rbinom(100, size = 1, prob = 0.5)
  m <- glm(y ~ 1, family = binomial)
  abs(plogis(coef(m)) - 0.5) < 0.1}
  )
mean(r)
```

This simple exercise gives us some sense of a reasonable sample size for just a simple model. We're looking at about n = 100.

The `pmsampsize` package can help us compute the minimum sample size required for the development of a new multivariable logistic regression prediction model.

We'll use the example that comes with the package:

Use `pmsampsize()` to calculate the minimum sample size required to develop a
multivariable prediction model for a binary outcome using 24 candidate predictor parameters. Based on previous evidence, the outcome prevalence is anticipated to be 0.174 (17.4%) and a lower bound (taken from the adjusted Cox-Snell R-squared of an existing prediction model) for the new model's R-squared value is 0.288

- `type = "b"` means "binomial outcome"
- `rsquared = 0.288` is the expected R-squared value of the new model. This is the percentage of variation in outcome values explained by the model.
- `parameters = 24` means our model may have as many as 24 coefficients (not variables but coefficients)
- `prevalence = 0.174` means we expect the overall proportion of subjects who experience the event of interest to be 0.174

The final answer is about 662 subjects.

```{r}
# installed.packages("pmsampsize")
library(pmsampsize)
pmsampsize(type = "b", rsquared = 0.288, parameters = 24, prevalence = 0.174)
```

### Assessing predictive accuracy 

How will our model perform for new data that was not used to create the model? _Model validation_ can help us assess this question.

A common approach to model validation is the _train/test data-splitting approach_, where we hold out a random subset of data (the test set), fit the model to the remaining data (the training set), and then see how well the model performs with the test data. However, this reduces the sample size used for model development and only validates a model fit to a subset of the data. There's also a chance a lucky split could produce unusually low or high predictive accuracy.

A more efficient approach is _bootstrap validation_. The basic idea is to resample your data with replacement, refit the model, then use the bootstrap model to calculate some statistic using the original data. 

The `rms` package provides the `validate` function for this purpose. However it only works for models fit using `rms` functions. Instead of `glm()`, we need to use `lrm()`. We also need to specify `x = TRUE, y = TRUE` so the original data is stored in the model object. To see a summary of the fit, we simply need to print the model object. Notice it returns more output than a model fit with `glm()`, including several discrimination indices.

```{r}
library(rms)
m_rms <- lrm(died ~ age + admit + uncons, 
                  data = icu, x = TRUE, y = TRUE)
m_rms
```

Now we can validate this model using the `validate` function. We set `B=200` to specify we want to perform 200 bootstrap resamples. Eleven statistics are calculated. 

- The first column `index.orig` are the original statistics using the fitted model on the data. 
- The last column `index.corrected` are the "corrected" statistics based on the bootstrap validation. 
- The `optimism` column summarizes how "optimistic" the original statistic was. The `index.corrected` value is simply the difference between `index.orig` and `optimism`.

We expect to see `index.corrected` values _smaller_ than the `index.orig` values, since the original values have been calculated using the same data to fit the model.

For example, the first row is Somer's D, a measure of discrimination. It takes values between -1 and 1. When Dxy=1, the predictions are perfectly discriminating. The original estimate was 0.6768. Bootstrap validation lowers it to about 0.6659 (at least when I ran it; bootstrapping is based on random resampling so you'll likely see a slightly different corrected value.) Likewise, the second value is R2, which measures the percentage of variation in outcome values explained by the model. We hope to see something close to 1. Originally it was 0.38, but the bootstrap validation lowers it to about 0.33.

```{r}
v <- validate(m_rms, B = 200)
v
```

For a summary of what all these statistics mean, see this excellent post at randomeffect.net: https://randomeffect.net/post/2021/05/02/the-rms-validate-function/

See also a blog post I wrote on this topic: https://data.library.virginia.edu/getting-started-with-bootstrap-model-validation/

Another type of bootstrap validation is _Calibration_. Calibration is the ability of a model to make unbiased estimates of an outcome. The idea is we would like our model to make accurate predictions for a new set of data not used to develop the model. If we don't have new data, we can use bootstrap resampling. Again the `rms` package makes this relatively easy to carry out with the `calibrate` function. Hopefully the solid "Bias-corrected" line will be diagonal with an intercept of 0 and a slope of 1. That would indicate predicted probabilities are corresponding to actual probabilities. What we see below is that from 0.2 - 0.8 our model severely _under-estimates_ the actual probability. For example, when our model returns a predicted probability of 0.5 (on the x-axis), the actual probability is about 0.6. The reliability of this model for future data may be in doubt.

```{r}
# run entire chunk at once
cal <- calibrate(m_rms, B = 200)
plot(cal)
grid()
```



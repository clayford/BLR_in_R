---
title: "Binary Logistic Regression in R"
author: "Clay Ford, UVA Library"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
plot(cars)
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## CODE ALONG 0

Enter a new code chunk and run the code `rbinom(n = 10, size = 1, prob = 0.5)`. This generates random values from Binomial Distribution with trial size = 1 and probability = 0.5. (this is like flipping a fair coin 10 times)



## Load packages

We'll use the following packages in this workshop.

```{r}
library(vcdExtra)
library(car)
library(ggeffects)
```


## Proportions and Probability

Pick a UVA student at random. What is the probability they have at least one tattoo? This is a _binary_ response. Only two possible answers: yes or no. We don't know, so we take a random sample. Let's say we randomly sample 100 students and find that 15 students have tattoos. 

The _proportion_, 15/100 = 0.15, could be interpreted as an estimated _probability_ that a UVA student has a tattoo. Recall that proportions and probabilities range from 0 to 1. We might conclude about 15% of UVA students have tattoos, or there's a probability of about 0.15 that a randomly selected UVA student has a tattoo.

What if we sampled 100 males and 100 females and compared the proportion with tattoos? Perhaps 0.18 of males have tattoos and 0.12 of females have tattoos. This suggests the probability of having a tattoo may be _related to gender_.

What if we also collected other information such as cumulative GPA or Greek life (yes or no)? Again this suggests the probability of having a tattoo may be related to multiple _variables_ or _predictors_.

_Binary Logistic Regression_ is a method that allows us to investigate questions such as this. It allows us to _model the probability_ of a binary event given multiple predictors.

Statistical modeling boils down to 3 steps:

1. propose and fit a model
2. assess if model is good
3. use model to make predictions or quantify relationships

## Load Data for the workshop

Today we'll work with data on Intensive Care Unit (ICU) admissions. This data are a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult ICU (Hosmer, Lemeshow and Sturdivant 2013) and included with the {vcdExtra} package, (Friendly 2022).

Below we read in the data from GitHub where it is stored as an _rds_ file. The rds format is an efficient way to store R objects. To read in an rds file we use the `readRDS()` function. To read in from a web site, we need to use the `url()` function to open a connection to the site.

```{r}
URL <- "https://github.com/clayford/BLR_in_R/raw/main/data/icu.rds"
icu <- readRDS(file = url(URL))
names(icu)
```

### List of variables

- died: Did patient die in ICU? (No or Yes)
- age: Patient age
- sex: Patient sex (Female or Male)
- cancer: Cancer part of present problem? (No or Yes)
- systolic: Systolic blood pressure at admission (mm Hg)
- admit: Type of admission (Elective or Emergency)
- uncons: patient unconscious at admission? (No or Yes)

The goal today is to learn the basics of Binary Logistic Regression by developing a model to predict the probability of death after admission to this ICU and to study the risk factors associated with ICU mortality. 


## Explore the ICU Data

The response variable is "died". Let's count the responses using the `xtabs()` function. The argument `addNA = TRUE` reports missing values if any are present. `xtabs()` uses formula notation. `~ died` means count the unique values of `died`.

```{r}
xtabs(~ died, data = icu, addNA = TRUE)
```

We can get the proportion of each category by using the `proportions()` function. Notice we can "pipe" the result of `xtabs()` into `proportions()` using the base R pipe operator, `|>`. (The base R pipe operator was introduced in R version 4.1.) A keyboard shortcut to insert the pipe operator is *Ctrl + Shift + M* (Win/Linux) or *Cmd + Shift + M* (Mac)

```{r}
xtabs(~ died, data = icu) |> proportions()
```

Just based on this information, a naive estimate of the probability of death (Yes) after admission to the ICU is about 0.20. But this probability estimate may change based on other predictors such as patient age, sex, whether or not patient was conscious at ICU admission, etc.

How does being unconscious at time of admission to ICU affect proportion of deaths? This time we use `xtabs()` to create a cross tabulation, and then pipe into `proportions()` with `margin = 1` to specify we want to calculate proportions for the rows. In other words, find proportion of `died` conditional on `uncons`. Of those who were unconscious at admission about 87% died. Only 15 out of 200 people were unconscious at admission.

```{r}
xtabs(~ uncons + died, data = icu) |>
  proportions(margin = 1) |>
  round(2)
```

How does age relate to the proportion of died? Plotting a factor as a function of a numeric variable using `plot()` creates a _spineplot_. The x-axis groups age into categories. Each vertical box shows the proportion of died within each age category. The width of the box corresponds to the proportion of age categories. Age 60 - 70 appears to be the largest group. It appears the proportion of deaths creeps up with increasing age.

```{r}
plot(died ~ age, data = icu)
```

A nice complement to the spineplot is the _stripchart_, which is basically one-dimensional scatter plot. This time we plot age as a function of died using the syntax `age ~ died`. Set `method = "jitter"` to prevent over-plotting.

```{r}
stripchart(age ~ died, data = icu, method = "jitter")
```


## CODE ALONG 1

- create a cross tabulation of admit and died
- Plot `died` as a function of `systolic` (spineplot).
- Plot `systolic` as a function of `died` (stripchart).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).



## Binary Logistic Regression basics

Binary logistic regression produces a _model_ that we can use to make probability predictions or quantify relationships. For example, with the ICU data we might...

- predict probability of death given age, sex, and whether you were unconscious at admission
- determine how the probability of dying changes if you were unconscious at admission

_We are responsible for proposing and assessing the model._ For example, the "sex" variable may or may not help our model. We have to decide to include it or not. Subject matter expertise is just as important as statistical expertise.

The most common way to fit binary logistic regression models in R is with the `glm()` function. GLM = Generalized Linear Model.

Let's fit a model. Notice we must set `family = binomial` because our response is binary. We almost always want to save the model result. The formula says "model `died` as a function of `age`, `sex`, and `uncons`." Call `summary()` on the model object to see the result.

```{r}
m <- glm(died ~ age + sex + uncons, data = icu, family = binomial)
summary(m)
```

The model is in the _Estimate_ column of the coefficients section. Here's the model expressed in basic math notation, with values rounded to two decimal places:

$$\text{died} = -3.58 + 0.03\text{ age} + 0.14\text{ sex} + 3.60\text{ uncons}$$

If we plug in values for `age`, `sex`, and `uncons` we get _predicted log odds_ of dying, not probability. 

Log odds are also known as _logit_ values. The logit transformation takes probability, which ranges from [0,1], and expresses it as log odds, which ranges from [-Inf,+Inf]. This is desirable since modeling probability directly could produce predictions below 0 or above 1.

Let's use our model to make a prediction for a male age 70 who was unconscious at admission. We use the `predict()` function with a data frame containing our predictors.

```{r}
predict(m, newdata = data.frame(age = 70, sex = "Male", uncons = "Yes"))
```

This obtained by plugging those values into our model:

$$2.145 = -3.58 + 0.03(70) + 0.14(1) + 3.60(1)$$

The prediction, 2.145, is hard to interpret. To get probability we need to take the _inverse logit_, which converts log odds back to the probability scale. Fortunately we can just set `type = "response"` in the `predict()` function.

```{r}
predict(m, newdata = data.frame(age = 70, sex = "Male", uncons = "Yes"), 
        type = "response")
```

How did we get that probability? The formula for the _inverse logit_ is as follows:

$$\text{Probability} =  \frac{e^x}{1 + e^x}$$

We can verify this is how we converted log-odds to probability.

```{r}
exp(2.145336)/(1 + exp(2.145336))
```

We predict a very high probability, about 0.90, of dying given you're a 70 year old male who was unconscious at admission to the ICU.

Again we should assess this model and think carefully before we fall in love with our prediction! We'll do this shortly.

Let's review the stuff at the bottom of the model summary:

```
(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 200.16  on 199  degrees of freedom
Residual deviance: 159.36  on 196  degrees of freedom
AIC: 167.36

Number of Fisher Scoring iterations: 5

```

- `(Dispersion parameter for binomial family taken to be 1)` This is a _GLM dispersion parameter_ that can be used to increase the standard errors of the coefficients. Since it's set to 1, the standard errors are not increased. This comes into play if we fit _quasibinomial_ models, which are sometimes used when a binomial response is more variable than expected (overdispersion).
- `Null deviance: 200.16` This is the sum of squared deviance residuals for a model with just an intercept and no predictors (i.e., proportion of died). This is meant to be compared with...
- `Residual deviance: 159.36` This is the sum of squared deviance residuals for the model we fit. Ideally it will be _substantially lower_ than the Null deviance. "When k [informative] predictors are added to a model, we expect deviance to decrease by more than k." (Gelman and Hill, 2006) In this case we'd like to see residual deviance less than 197, which we do.
- `AIC: 167.36` This refers to Akaike's Information Criterion. By itself it doesn't mean anything. We use it to compare models. Lower AIC is better. More on this later. 
- `Number of Fisher Scoring iterations: 5` This refers to the iterative numerical algorithm that estimated the parameters. The default maximum iterations is 25. This model only required 5 iterations. There's is purely informational. 



## Probability, Odds and Log Odds

Let p be probability.    
Odds = p/(1-p).     
Log odds is log(odds).   
 
Let's convert a few probabilities to odds and log odds. The function `fractions()` from the {MASS} package returns fractions, which is usually how odds are expressed. (We use `as.character()` to print the fraction bar.) Notice a few things:

- odds are always positive
- log odds can be positive or negative
- small changes in probability can result in "big" changes in log odds. log odds change rapidly when p is close to 0 or 1. _In general, never a good idea to round log odds, especially when dealing with small probabilities. _

```{r}
p <- c(0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 0.999)
odds <- MASS::fractions(p/(1-p))
logodds <- log(odds)
data.frame(p, odds = as.character(odds), logodds)
```

No intro to logistic regression would be complete without a plot of probability versus log odds. Notice that no matter how big or small the log odds, the corresponding probability is always between 0 and 1.

```{r}
p <- seq(0.0001, 0.9999, length.out = 1000)
logodds <- log(p/(1-p))
plot(logodds, p, type = "l")
```


R has built-in functions for converting probability to log odds and log odds to probability. 

- `qlogis`: convert probability to log odds (logit)
- `plogis`: convert log odds to probability (inverse logit)

Convert log odds of 2 to probability.

```{r}
# inverse logit
plogis(2.145)
```

Convert probability of 0.8952006 to log odds.

```{r}
# logit
qlogis(0.8952006)
```

You probably don't need these functions when doing logistic regression in R, but they can be useful to know. 

## How to interpret coefficients

Our model is comprised of simple additive predictors, or _main effects_. Therefore the coefficients can be interpreted. We can use the `coef()` function to extract the coefficients.

```{r}
coef(m)
```

These coefficients are on the log odds scale. For example, the age coefficient says the log odds of ICU death increase by about 0.028 for every one year increase in age, holding all other variables constant. 

If we _exponentiate the coefficients_ we get _odds ratios_. An odds ratio is a ratio of two odds. For example if we exponentiate the age coefficient, we get 1.028719. 

```{r}
exp(coef(m)["age"])
```

This says the odds of ICU death increase by about 1.029 for each one year increase in age. Put another way, the odds of dying when you're 70 are about 3% higher than the odds of dying when you're 69, holding all other variables constant. (Recall that multiplying by 1.03 is the same as increasing by 3%.)

Let's do an odds ratio "by hand".

Say an event has a probability of 0.8 of happening. The corresponding odds are 4, or 4 to 1. We can expect 4 "successes" for every one "failure". 

```{r}
# event 1
p1 <- 0.8
odds1 <- p1/(1-p1)
odds1
```

Say another event has a probability of 0.6 of happening. The corresponding odds are 1.5.

```{r}
# event 2
p2 <- 0.6
odds2 <- p2/(1-p2)
odds2 
```

The odds ratio of the two events is 4/1.5, which is about 2.7. 

```{r}
odds1/odds2
```

This says the odds of event 1 are about 2.7 times higher than the odds of event 2.

An odds ratio of 1 implies the odds are the same, and hence there is no difference between the event probabilities. They are equally likely.

Let's look at the odds ratios of our model:

```{r}
exp(coef(m))
```

Our model claims the odds of dying when unconscious at admission are about 36 times higher than the odds of dying when you are not unconscious at admission! We can calculate this "by hand" by predicting probabilities for, say, two Males aged 60, one who was unconscious and the other who was not:

```{r}
# unconscious male, age 60
p1 <- predict(m, newdata = data.frame(age = 60, sex = "Male", 
                                      uncons = "Yes"),
              type = "response")
# conscious male, age 60
p2 <- predict(m, newdata = data.frame(age = 60, sex = "Male", 
                                      uncons = "No"),
              type = "response")
c(p1, p2)
```

```{r}
# convert probabilities to odds
odds_p1 <- p1/(1-p1)
odds_p2 <- p2/(1-p2)
c(odds_p1, odds_p2)
```

```{r}
# calculate odds ratio
or <- odds_p1/odds_p2
or
```

The odds ratio for age is about 1.03. This says the odds of dying increase about 3% for every 1 year increase in age. If we wanted to estimate that for every 10 years increase in age, we simply multiply the coefficient by 10.

```{r}
exp(coef(m)["age"] * 10)
```

Every 10 year increase in age increases the odds of dying by about 33%, all else held constant.

Odds ratios only express _relative_ differences. A large odds ratio does not mean _absolute_ probabilities are large. It can help to take the _difference in predicted probabilities_ to put odds ratios in perspective.

Let's revisit this statement: "Every 10 year increase in age increases the odds of dying by about 33%, all else held constant."

Predicted probability a female age 40 who arrives conscious dies in ICU:

```{r}
pred1 <- predict(m, 
                 newdata = data.frame(age = 40, sex = "Female", 
                                      uncons = "No"),
                 type = "response")
pred1
```

Predicted probability a female age 50 who arrives conscious dies in ICU:

```{r}
pred2 <- predict(m, 
                 newdata = data.frame(age = 50, sex = "Female", 
                                      uncons = "No"),
                 type = "response")
pred2
```

Verify odds ratio from above by literally calculating odds ratio:

```{r}
(pred2/(1 - pred2))/
  (pred1/(1 - pred1))
```

But look at the predicted probabilities and absolute difference. They're all quite low.

```{r}
cbind("age 50" = pred1, "age 40" = pred2, "Diff" = pred2 - pred1)
```


## Standard errors and hypothesis tests

If we use `coef()` on `summary()` we extract the coefficient table which contains standard errors, test statistics (z value), and p-values.

```{r}
coef(summary(m)) |> round(3)
```

- Std. Error: quantifies uncertainty of estimated coefficient. It may help to imagine adding and subtracting it to your estimate to get an idea of the variability of your estimate.
- z value: Ratio of Estimate to Std. Error. Large ratios mean coefficients with relatively small standard errors. z values larger than 2 or 3 are usually considered "big".
- Pr(>|z|): probability of getting a value as big or bigger than |z| if coefficient is 0, given other predictors already in model. This two-sided probability is calculated using a standard Normal distribution, N(0,1).

Altogether this information is used to assess whether predictor coefficients are reliably positive or negative (i.e., different from 0)

P-values only assess whether a coefficient is different from 0. It's better to assess _direction and magnitude_ of coefficients with _confidence intervals_. Use `confint()` on the model object.

```{r}
confint(m)
```

We're not sure if being Male increases or decreases the chance of dying.  

Using `exp()` with `confint()` provides a _confidence interval on the odds ratios_. Of interest here is whether or not the CI crosses 1. A ratio of 1 implies no difference in odds.

```{r}
exp(confint(m))
```

The effect of age seems small, at least per one year increase. The effect of being unconscious seems highly variable. That's because we only have 15 subjects who arrived unconscious.

## CODE ALONG 2

- Model died as a function of age, admit, and uncons. Save the model as `m2`.
- Use `summary()` and `confint()` on the model
- Interpret the coefficient for admit as an odds ratio
- What's the predicted probability of dying for a person age 50 with Emergency admission and unconscious?

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).



## Model assessment

It's easy to use `glm()` to fit a binary logistic model, but is the model "good"? Is one model better than another? How do we know which predictors to keep and which ones to drop?  _Model building is hard_ and requires equal parts statistical expertise and subject matter expertise. 

### (1) Is our model better than the NULL model?

One question to consider: _is our model better than the NULL model_, where the NULL model is simply the proportion of "successes". Recall 20% of the patients admitted to the ICU died.

```{r}
# proportion of died = "Yes"
mean(icu$died == "Yes")
```

The NULL model is simply predicting anyone admitted to the ICU has a 0.20 probability of dying. This is sometimes called the _intercept-only model_, which we can fit as follows:

```{r}
# died ~ 1 is the intercept-only model
m0 <- glm(died ~ 1, data = icu, family = binomial)
coef(m0)
```

The intercept coefficient is the predicted _log odds_ of dying. We can convert log odds to probability using the inverse logit, which is available as the `plogis()` function. This turns out to be a fancy way to get 0.2.

```{r}
# convert log odds to probability
plogis(coef(m0))
```

We can formally assess if a proposed model is better than the NULL model using the base R `anova()` function. Recall our model `m`

```{r}
formula(m)
```

How much better is it than the NULL model? Call `anova()` on your model object:

```{r}
anova(m)
```

Notice that terms are added sequentially, _in order_. This is sometimes called a _Type I test_.

- A model with just age appears to be better than the NULL model. 
- Adding sex with age already in the model doesn't seem to make any further improvement. 
- Adding uncons with age and sex already in the model further improves the model quite a bit. Notice the drop in _Residual Deviance_. 

Deviance is a _statistical summary of model fit_. A "good" predictor should drop Residual Deviance by more than 1. Notice sex produces no change in deviance, but uncons drops it more than 30.

See this article for an explanation of what exactly deviance residuals are:
<https://library.virginia.edu/data/articles/understanding-deviance-residuals>

An alternative to Type I tests are _Type II tests_, which test the contribution of each variable _assuming all others are included_. The base R `drop1()` function produces Type II tests. This technically does not compare the model to the NULL model. However it does allow us to asses each variable's contribution to the model.

```{r}
drop1(m, test = "Chisq")
```

The substance of the results are largely the same, however each test assumes the other variables are in the model. For example, adding age to a model with sex and uncons already present appears to improve the model.

### (2) Is our model worse than the SATURATED model?

A second question to consider: _how bad is our model compared to a saturated model that fits the data perfectly_? The `LRstats()` function from the {vcdExtra} package helps us to assess this question. It conducts a hypothesis test with the Null being "the model fit is no different from the saturated model". A low p-value provides evidence against this hypothesis.

```{r}
LRstats(m)
```

The result of this test fails to provide evidence against the Null. That's good.

See also the "Assessing predictive accuracy" section in the appendix of this notebook for more information. 

## CODE ALONG 3

Assess model `m2` using the methods we just described.

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).


### Fitting interactions

So far our models only contain _main effects_. The effects are _additive_. It doesn't matter what age you are, the effect of uncons is the same. 

We can relax that assumption by allowing variables to _interact_. These make our models more complex and more difficult to interpret. But they also may produce better models that more accurately explain the probability of our response.

Note: more complex models require more data. See the _Minimum sample sizes_ section in the appendix of this Notebook.

When variables interact, that means we cannot untangle their effects. The effect of one _depends_ on another. For example, perhaps the effect of age depends on uncons status. 

Below we allow age and uncons to interact by adding `age:uncons` to the model. That does not mean they really interact. We're simply _allowing them to interact in our model_. The evidence for the hypothesized interaction seems weak judging from the p-value (0.11).

```{r}
m_int <- glm(died ~ age + sex + uncons + age:uncons, data = icu,
             family = binomial)
summary(m_int)
```

The confidence interval on the interaction barely overlaps 0. There may be an interaction. Of course we only have 15 subjects who arrived unconscious.

```{r}
confint(m_int)
```

Let's fit a model where age and systolic interact. The p-value for the interaction appears "significant", but the interaction itself appears to be tiny!

```{r}
m3 <- glm(died ~ age + systolic + age:systolic, data = icu, 
          family = binomial)
summary(m3)
```

The confidence interval on the interaction does not overlap 0, but at it's largest it's only about -0.002.

```{r}
confint(m3) |> round(5)
```

Is this interaction important? How can we assess this?

## Visualizing models

_Effect plots_ allow us to visualize our model. 

The basic idea is to pick a variable of interest, say age, and make predictions for it over a range of values while holding other predictors constant.

Below we show how to do this using the `predict_response()` function from the {ggeffects} package. 

Lets visualize model `m3`. The first argument is our model object, the second argument specifies which variables, or model terms, we wish to visualize. This produces predictions at various ages for specific values of systolic.

```{r}
predict_response(m3, terms = c("age", "systolic")) 
```

We can pipe this result into `plot()` to get a basic effect plot. Notice we can define the range of the data by appending `[30:80]` to age. The colored ribbons are 95% confidence bands. It appears as systolic gets lower the effect of age on death increases.

```{r}
predict_response(m3, terms = c("age[30:80]", "systolic"))  |> 
  plot()
```


The values of systolic are automatically set to the mean and +/- one standard deviation. We can manually set those values if we like as follows. Note this syntax is unique to the {ggeffects} package.

```{r}
predict_response(m3, terms = c("age[30:80]", "systolic[95, 130, 165]")) |>
  plot()
```


## CODE ALONG 4

- Fit a new model with the following formula and save as `m4`: 
  `died ~ age + admit + uncons + age:uncons`   
- Visualize the interaction.

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).


## Checking influence of observations

One or two influential observations can completely change a model. For example, a notable interaction may be due to an observation's unusual values. 

An effective plot for quickly assessing influence is the `influencePlot()` function from {car} package. It plots three different diagnostics:

1. _residuals_ on y axis: how different is an observed response from its predicted response
2. _Hat values_ on x axis: measures "leverage", potential impact of a case on the model
3. _Cook's distance_ as size of points: influence of observation on regression coefficients

Let's fit a model with all predictors and assess the model for influential observations. Let's call the model `m_all`.

```{r}
m_all <- glm(died ~ ., data = icu, family = binomial)
summary(m_all)
```

Now call `influencePlot()` on the model object. A plot is produced with "noteworthy" points labeled. Assigning the result creates a data frame of the most noteworthy values. 

```{r}
infl <- influencePlot(m_all)
```

Below are the exact values of the labeled points. 

- A Cook's D bigger than 1 is the usual red flag. 
- A Hat Value is considered large if it exceeds (2 or 3)(k/n) where k is the number of model coefficients and n is number of observations. Here that comes out to (2 or 3)(7/200) or about (0.07 - 0.11). 
- Residuals larger than 2.5 may be of interest.

```{r}
infl
```

We can extract the rownames and use them to subset the icu data frame to see the observed values. 

```{r}
pred <- predict(m_all, type = "response")[rownames(infl)]
obs <- icu[rownames(infl),]
cbind(obs, pred)
```

- Subject 18: model predicts death at 97%, but subject lived
- Subject 172: subject had Elective admit but arrived unconscious? (typo?)
- Subject 181: model predicts death at 4%, but subject died
- Subject 200: subject had systolic value of 256 (the max)

One action to take based on these results is a _sensitivity analysis_ where we fit the model without one or more of the data points to see how the model changes. Let's drop subject 18 since they have the highest Cook's D value. Then we compare coefficients from both models using the `compareCoefs()` function from the {car} package.

```{r}
m_all2 <- glm(died ~ ., data = icu, family = binomial, 
          subset = -18)
compareCoefs(m_all, m_all2)
```

By dropping subject 18, the coefficient of the uncons main effect increases from 3.9 to 5.4. This doesn't necessarily mean we should remove this subject from the analysis. It just means they're influencing the model and we should exercise caution and modesty in drawing conclusions.


## We're done!

See the appendix for more material cut for time.

For help with statistical analysis, contact the UVA Library StatLab: `statlab@virginia.edu`

## References

-  Fox J, Weisberg S (2019). _An R Companion to Applied Regression_, Third edition. Sage, Thousand Oaks CA. <https://socialsciences.mcmaster.ca/jfox/Books/Companion/>.
- Friendly M, Meyer D (2016). Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data. CRC Press.
- Friendly M (2022). _vcdExtra: 'vcd' Extensions and Additions_. R package version 0.8-0, <https://CRAN.R-project.org/package=vcdExtra>.
- Gelman A, Hill J (2006). _Data Analysis Using Regression and Multilevel/Hierarchical Models_. Cambridge University Press.
- Lemeshow S, Teres D, Avrunin JS, Pastides H. (1988). Predicting the Outcome of Intensive Care Unit Patients. Journal of the American Statistical Association, 83, 348-356.
- LÃ¼decke D (2018). "ggeffects: Tidy Data Frames of Marginal Effects from Regression Models." _Journal of Open Source Software_, *3*(26), 772. doi:10.21105/joss.00772  <https://doi.org/10.21105/joss.00772>.
- R Core Team (2022). R: A language and environment for statistical  computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
- Harrell F (2015). Regression Modeling Strategies, 2nd ed. Springer. http://hbiostat.org/rmsc/


## Appendix


## Confidence intervals on predictions

A prediction is just a point estimate. There is some uncertainty in the estimate. A confidence interval helps quantify the uncertainty. Unfortunately the `predict()` function for binary logistic regression models does not provide a confidence interval for predictions. However we can calculate it ourselves.

Earlier we made a prediction for a male, age 60 who was admitted unconscious to the ICU as follows:

```{r}
predict(m, newdata = data.frame(age = 60, sex = "Male", uncons = "Yes"),
              type = "response")
```

While there is no option for a confidence interval, we can request the standard error of the prediction by specifying `se.fit = TRUE`. Notice below we request the predicted log odds instead of the response. This is so we get a standard error on the log odds scale. This will prevent us from having  confidence interval that goes below 0 or above 1. We save to object "pred" for convenience.

```{r}
pred <- predict(m, newdata = data.frame(age = 60, sex = "Male", 
                                        uncons = "Yes"),
                se.fit = TRUE)
pred
```

Now we add and subtract 1.96 standard errors to the prediction to get a 95% confidence interval on the log odds scale, and then use the inverse logit to transform to probabilities.

```{r}
plogis(pred$fit + c(-1,1)*1.96*pred$se.fit)
```

An easier way is to use the `predict_response()` function from the {ggeffects} package to calculate that confidence interval. Instead of supplying new data in data frame we use the `terms` and `condition` arguments. The `terms` are the variables of interest. The `condition` are the variables we want to condition on. Below we arbitrarily set age as the term and sex and uncons as the conditions. `age[60]` says set age to 60. `condition = c(sex = "Male", uncons = "Yes")` says to hold those two variables at "Male" and "Yes", respectively.

```{r}
predict_response(m, terms = "age[60]", 
          condition = c(sex = "Male", uncons = "Yes"))
```

This is admittedly a little weird. `predict_response()` is usually employed to generate _many_ predictions for a term of interest. For example:

```{r}
predict_response(m, terms = "age[30:60 by=5]", 
          condition = c(sex = "Male", uncons = "Yes"))
```

Below we show how to visualize binary logistic regression models using the {ggeffects} package, which calculates confidence intervals for predictions using the same method we just demonstrated. 


## Comparing models

We can compare models by using criteria such as AIC and BIC as well as hypothesis tests. Let's review both.

AIC and BIC are statistics for measuring goodness of fit. In isolation these values are not meaningful, but when compared for different models with the same response, _lower values are better_. The `LRstats` function shows both criteria. Let's compare models `m` and `m2`:

```{r}
formula(m)
formula(m2)
LRstats(m, m2)
```

Even though both models appear to be just as good as a saturated model, `m2` appears to better than `m`.

When one model is _nested_ in another, we can use hypothesis tests to assess whether the simpler model is just as good as the more complex model. Let's fit two models, one nested in the other.

Model 1: age and uncons  
Model 2: age, uncons and their interaction

Model 2 says the effect of age depends on uncons and vice versa. Model 1 is nested within Model 2 since it's a special case of Model 2 with an interaction coefficient of 0.

```{r}
m4 <- glm(died ~ age + uncons, data = icu, family = binomial)
m5 <- glm(died ~ age + uncons + age:uncons, data = icu, family = binomial)
```

Now we can carry out a formal test to compare the models using the `anova()` function. The null hypothesis is both models fit equally well. A low p-value provides evidence against the null.

```{r}
anova(m4, m5, test = "Chisq")
```

The interaction may be useful but is probably small. Whether or not to keep it is just as much a subject matter question as it is a statistical one. Comparing AIC and BIC values is also inconclusive.

```{r}
LRstats(m4, m5)
```


### Fitting non-linear effects

A non-linear effect is not constant. The slope or trajectory changes over the range of values. For example, humans don't grow in height at a constant rate. We don't grow 3 inches per year until we die. We tend to grow at varying rates until we stop in our late teens or 20s.

Below we allow age to have a non-linear effect by using a _natural spline_ with 2 degrees of freedom. This basically allows the effect of age to change trajectory twice if needed. It's somewhat similar to polynomial regression, but better behaved in the extremes of the data. The splines package comes with R. The `ns()` function can be used directly in the model formula. The second argument specifies how many times you want to let the effect "bend", or change direction. 

```{r}
library(splines)
m_nle <- glm(died ~ ns(age, 2) + sex + uncons, data = icu,
             family = binomial)
summary(m_nle)
```

It's hard to tell if the spline is necessary based on the summary output. One proper way to assess this is using the `anova()` function, where we can compare model with a linear effect age to the model with a nonlinear effect of age. This tests whether or not we need the non-linear term. The null hypothesis of this test is that both models are equally good, which implies we don't need the non-linear effect for age.

```{r}
anova(m, m_nle, test = "Chisq")
```

It appears we don't benefit from the non-linear effect.


## Making comparisons

In addition to visualizing our model results, we might like to make some follow-up comparisons. For example, the predicted probability of death when admitted to the ICU conscious is about 0.13. The probability of death when admitted unconscious is about 0.85. That's a difference of about 0.72. How certain are we about that difference? For this we would like to calculate a confidence interval. 

We can use the {emmeans} package for this task. The argument `regrid = "response"` means use the original response scale (probability) for the "reference grid". This gives us a 95% confidence interval on the difference in probabilities: [0.52, 0.91]

```{r}
library(emmeans)
emmeans(m, specs = "uncons", regrid = "response") |> 
  pairs(reverse = TRUE) |> 
  confint()
```

Instead of working with absolute difference in probabilities, we can work with relative differences. To do this, we set `type = "response"`. Notice we get an odds ratio, which is the same odds ratio we got when we exponentiated the coefficient for uncons.

```{r}
emmeans(m, specs = "uncons", type = "response", 
        at = list(sex = "Female", age = 63)) |> 
  pairs(reverse = TRUE) |> 
  confint()
```

Finally let's work with model `m3`. Let's say we want to compare the probability of death at age 50 versus age 60 conditional on systolic levels of 95, 130 and 165.

```{r}
emmeans(m3, specs = ~ age | systolic, 
        at = list(age = c(50, 60), 
                  systolic = c(95, 130, 165)), 
        regrid = "response") |> 
  pairs(reverse = TRUE)
```

It appears there's a notable difference when systolic = 95, and that difference becomes less pronounced as systolic increases.




### Three ways to fit a binary logistic regression model

The `glm()` function allows binary response data to be structured in three different ways:

1. As a 0/1 indicator or factor. This is what we presented in this workshop
2. As a proportion with the total number of cases given by the `weights` argument.
3. As a two-column matrix, with the first column giving the number of successes and the second the number of failures.

Let's demonstrate all three ways using a subset of the icu data.

#### 1. As a 0/1 indicator

Let's say we want to model died as function of uncons and age. Our data is already structured as one row per event with a 0/1 indicator.

```{r}
mod1 <- glm(died ~ uncons + age, data = icu, family = binomial)
summary(mod1)
```

#### 2. As a proportion

To demonstrate this we need to first restructure the data. To begin, we need to set the response as numeric 0 and 1. The died variable is currently a factor.

```{r}
icu$died <- ifelse(icu$died == "Yes", 1, 0)
```

Next we get the total deaths for each uncons and age combination. For example, we have 3 deaths for subjects age 75 who were not unconscious upon submission to the icu.

```{r}
# sum of died by uncons and age
x <- aggregate(died ~ uncons + age, data = icu, sum)
# look at rows for ages 72 - 75
x[x$age %in% 72:75,]
```

Next we get the total number of observations for each age and uncons combination. For example, we have 2 observations for subjects age 16 who came to the icu conscious.

```{r}
# sum of observations by uncons and age
n <- aggregate(died ~ uncons + age, data = icu, length)
head(n)
```

Finally we combine the data into a data frame. We name the number of observations for each uncons/age combination as "trials" and the proportion of deaths per combination as "proportion".

```{r}
# combine into data frame and create column for proportions
icu2 <- data.frame(uncons = x$uncons, 
                   age = x$age, 
                   died = x$died, 
                   trials = n$died,
                   proportion = x$died/n$died)
head(icu2)
```

Now we use "proportion" as the response, but also include the argument `weights = trials` to indicate total number of observations at each uncons and age combination.

```{r}
mod2 <- glm(proportion ~ uncons + age, data = icu2, family = binomial,
            weights = trials)
summary(mod2)
```

Notice the coefficient estimates are identical but the deviance statistics are different.

We could also use trials and died on-the-fly by entering `died/trials` as the response, like so:

```{r}
mod2 <- glm(died/trials ~ uncons + age, data = icu2, family = binomial,
            weights = trials)
summary(mod2)
```


#### 3. As a two-column matrix

In this scenario, the response is a two-column matrix, with the first column giving the number of deaths and the second the number of survivors. We need to add number of survivors to our data frame, which is simply the difference between trials and died.

```{r}
icu2$survivor <- icu2$trials - icu2$died
```

Now use `cbind()` to create a matrix of "died" and "survivor" and use that as our response. We no longer need to use the weights argument in this case.

```{r}
mod3 <- glm(cbind(died, survivor) ~ uncons + age, 
            family = binomial, data = icu2)
summary(mod3)
```

The result for the third method is identical to the result for the second method.

_Why one method versus the other?_

Even though all three methods produce the same model coefficients, method 1 produces residuals that differ from methods 2 and 3.

Method 1 produces a residual for each unique predictor value. The resulting residual versus fitted plot produces weird patterns that make it hard to assess model fit. Below is a plot for `mod1`. In this case a residual is `1 - predicted probability` or `0 - predicted probability` depending on the observed response. 

```{r}
plot(mod1, which = 1)
```

Compare to a residual versus fitted plot for `mod2` and `mod3`. This looks similar to the types of residual versus fitted plots we get for linear models. 

```{r}
plot(mod2, which = 1)
```

rows 39 and 74 of icu2 reveal two older patients who came to icu unconscious but lived. The model predicts a high probability of them dying, but they did not. Hence the low, negative position in the plot. We observe them living but the model predicts high probability of death. The residual is therefore negative. The plot suggests these subjects had a lower probability of death than the model predicts. 

```{r}
icu2$fitted <- fitted(mod2)
icu2[c(39,74),]
```


### Simulating data for logistic regression

It can be useful to know how to simulate binary data. This can aid in preparing for a study or making power and sample size estimates.

Pretend we want to simulate data for the binary event of a UVA student having a tattoo with probability associated with age and whether or not parent/guardian has tattoo. First generate age and pt (parent has tattoo) variables. Assume a sample size of 200. We'll center the age variable so 0 is the mean.

```{r}
n <- 200
set.seed(1)
age <- sample(18:30, size = n, replace = TRUE)
age_c <- age - mean(age) # center age so mean is 0.
pt <- sample(c("no", "yes"), size = n, replace = TRUE)
```

Now use those variables to simulate a response on the _log odds scale_. Let's say each one year increase in age increases the odds of having a tattoo by about 10% and that having a parent with a tattoo increases the odds of having a tattoo by about 65%. This implies coefficients of 0.1 and 0.5, respectively, which I determined through some trial and error. We'll also assume an intercept of -1.73, which implies a probability of 0.15 for having a tattoo if you're the mean age and your parents had no tattoo.

```{r}
exp(0.1) # about 10% increase in odds
exp(0.5) # about 65% increase in odds
qlogis(0.15) # prob of tattoo at mean age and parents w/out tattoo
```

Now calculate tattoo_log_odds as follows.

```{r}
tatoo_log_odds <- -1.73 + 0.1*age_c + 0.5*(pt == "yes")
```

Next convert the log odds to probability using `plogis()`, which returns the inverse logit, or inverse log odds.

```{r}
tattoo_prob <- plogis(tatoo_log_odds)
```

Notice our probabilities range from  about 0.09 to 0.35.

```{r}
summary(tattoo_prob)
```

Now we can use those probabilities to return a 0 or 1 using the `rbinom()` function. This simulates data from a _binomial distribution_. We will use each individual probability to basically flip a coin with the stated probability and get a 0 or 1.

```{r}
set.seed(2)
tattoo <- rbinom(n = n, size = 1, prob = tattoo_prob)
```

Let's tally up our simulated events.

```{r}
xtabs(~ tattoo)
```

A student with a tattoo seems more likely to have a parent with a tattoo. That's according to our simulation specifications.

```{r}
xtabs( ~ tattoo + pt)
```

Likelihood of having a tattoo also increases with age. Again this is per our specification.

```{r}
plot(factor(tattoo) ~ age)
```


Now let's use `glm()` to work backwards and see how close we get to recovering the true coefficients we used to simulate the data.

```{r}
mod <- glm(tattoo ~ age_c + pt, family = binomial)
# True model: -1.73 + 0.1*age_c + 0.5*(pt == "yes")
summary(mod)
```

Pretty close, though there is a lot of uncertainty about the pt variable.

Let's pretend this model is the TRUTH about the population. Is 200 subjects a large enough sample to reliably detect the positive effect of a parent having a tattoo on the probability of a student having a tattoo?

We can simulate data and fit a model many times and then see the proportion of times we get a p-value on the pt variable that is below, say, 0.05. For example the following code returns TRUE/FALSE:

```{r}
coef(summary(mod))["ptyes", "Pr(>|z|)"] < 0.05
```

We can use the `replicate()` function with this code along with our code above to sample and fit many models and see if we detect the "pt" effect. Simply copy and paste the relevant portions of code from above into the {} for the `expr` argument. Use the `n` argument to specify the number of replications. 

```{r}
rep.out <- replicate(n = 500, expr = {
  n <- 200
  age <- sample(18:30, size = n, replace = TRUE)
  age_c <- age - mean(age) 
  pt <- sample(c("no", "yes"), size = n, replace = TRUE)
  tatoo_log_odds <- -1.73 + 0.1*age_c + 0.5*(pt == "yes")
  tattoo_prob <- plogis(tatoo_log_odds)
  tattoo <- rbinom(n = n, size = 1, prob = tattoo_prob)
  mod <- glm(tattoo ~ age_c + pt, family = binomial)
  coef(summary(mod))["ptyes", "Pr(>|z|)"] < 0.05
})
```

Now take the mean of rep.out, which is a vector of TRUE/FALSE values. This returns the proportion of times we reject the null of no "pt" effect. We might use this as an estimate of power.

```{r}
mean(rep.out)
```

It appears this "experiment" is not very powerful to detect the parent effect on having a tattoo, assuming our hypothesized coefficients are true. What if we increase the sample size to 1000?

```{r}
rep.out <- replicate(n = 500, expr = {
  n <- 1000
  age <- sample(18:30, size = n, replace = TRUE)
  age_c <- age - mean(age)
  pt <- sample(c("no", "yes"), size = n, replace = TRUE)
  tatoo_log_odds <- -1.73 + 0.1*age_c + 0.5*(pt == "yes")
  tattoo_prob <- plogis(tatoo_log_odds)
  tattoo <- rbinom(n = n, size = 1, prob = tattoo_prob)
  mod <- glm(tattoo ~ age_c + pt, family = binomial)
  coef(summary(mod))["ptyes", "Pr(>|z|)"] < 0.05
})
mean(rep.out)
```

This seems much better. But this assumes we have the resources to sample 1000 students.




### Minimum sample sizes

Harrell (2015) shows the minimum sample size required to estimate an intercept-only logistic regression model to within 0.1 of the true probability with 95% confidence is _96_, assuming true probability is 0.5.

We can show this using simulation. Below we simulate 50 observations using p = 0.5. We then fit an intercept only model and extract the estimated probability.

```{r}
y <- rbinom(50, size = 1, prob = 0.5)
m <- glm(y ~ 1, family = binomial)
plogis(coef(m))
```

Let's replicate this 1000 times and see how often we come within 0.1 of the true probability, 0.5. We simply copy-and-paste our code above in between `{}` and use that for the `expr` argument in the `replicate()` function. When I ran this I calculated about 0.85.

```{r}
r <- replicate(n = 1000, expr = {
  y <- rbinom(50, size = 1, prob = 0.5)
  m <- glm(y ~ 1, family = binomial)
  abs(plogis(coef(m)) - 0.5) < 0.1}
  )
mean(r)
```

Let's try n = 70. This returned about 0.90 for me.

```{r}
r <- replicate(n = 1000, expr = {
  y <- rbinom(70, size = 1, prob = 0.5)
  m <- glm(y ~ 1, family = binomial)
  abs(plogis(coef(m)) - 0.5) < 0.1}
  )
mean(r)
```

Finally let's try n = 100. This returned 0.952 for me, expressing that we came within 0.1 of the true value (0.5) about 95% of the time. 

```{r}
r <- replicate(n = 1000, expr = {
  y <- rbinom(100, size = 1, prob = 0.5)
  m <- glm(y ~ 1, family = binomial)
  abs(plogis(coef(m)) - 0.5) < 0.1}
  )
mean(r)
```

This simple exercise gives us some sense of a reasonable sample size for just a simple model. We're looking at about n = 100.

The `pmsampsize` package can help us compute the minimum sample size required for the development of a new multivariable logistic regression prediction model.

We'll use the example that comes with the package:

Use `pmsampsize()` to calculate the minimum sample size required to develop a
multivariable prediction model for a binary outcome using 24 candidate predictor parameters. Based on previous evidence, the outcome prevalence is anticipated to be 0.174 (17.4%) and a lower bound (taken from the adjusted Cox-Snell R-squared of an existing prediction model) for the new model's R-squared value is 0.288

- `type = "b"` means "binomial outcome"
- `rsquared = 0.288` is the expected R-squared value of the new model. This is the percentage of variation in outcome values explained by the model.
- `parameters = 24` means our model may have as many as 24 coefficients (not variables but coefficients)
- `prevalence = 0.174` means we expect the overall proportion of subjects who experience the event of interest to be 0.174

The final answer is about 662 subjects.

```{r}
# installed.packages("pmsampsize")
library(pmsampsize)
pmsampsize(type = "b", rsquared = 0.288, parameters = 24, prevalence = 0.174)
```

### Assessing predictive accuracy 

How will our model perform for new data that was not used to create the model? _Model validation_ can help us assess this question.

A common approach to model validation is the _train/test data-splitting approach_, where we hold out a random subset of data (the test set), fit the model to the remaining data (the training set), and then see how well the model performs with the test data. However, this reduces the sample size used for model development and only validates a model fit to a subset of the data. There's also a chance a lucky split could produce unusually low or high predictive accuracy.

A more efficient approach is _bootstrap validation_. The basic idea is to resample your data with replacement, refit the model, then use the bootstrap model to calculate some statistic using the original data. 

The `rms` package provides the `validate` function for this purpose. However it only works for models fit using `rms` functions. Instead of `glm()`, we need to use `lrm()`. We also need to specify `x = TRUE, y = TRUE` so the original data is stored in the model object. To see a summary of the fit, we simply need to print the model object. Notice it returns more output than a model fit with `glm()`, including several discrimination indices.

```{r}
library(rms)
m_rms <- lrm(died ~ age + admit + uncons, 
                  data = icu, x = TRUE, y = TRUE)
m_rms
```

Now we can validate this model using the `validate` function. We set `B=200` to specify we want to perform 200 bootstrap resamples. Eleven statistics are calculated. 

- The first column `index.orig` are the original statistics using the fitted model on the data. 
- The last column `index.corrected` are the "corrected" statistics based on the bootstrap validation. 
- The `optimism` column summarizes how "optimistic" the original statistic was. The `index.corrected` value is simply the difference between `index.orig` and `optimism`.

We expect to see `index.corrected` values _smaller_ than the `index.orig` values, since the original values have been calculated using the same data to fit the model.

For example, the first row is Somer's D, a measure of discrimination. It takes values between -1 and 1. When Dxy=1, the predictions are perfectly discriminating. The original estimate was 0.6768. Bootstrap validation lowers it to about 0.6659 (at least when I ran it; bootstrapping is based on random resampling so you'll likely see a slightly different corrected value.) Likewise, the second value is R2, which measures the percentage of variation in outcome values explained by the model. We hope to see something close to 1. Originally it was 0.38, but the bootstrap validation lowers it to about 0.33.

```{r}
v <- validate(m_rms, B = 200)
v
```

For a summary of what all these statistics mean, see this excellent post at randomeffect.net: https://randomeffect.net/post/2021/05/02/the-rms-validate-function/

See also a blog post I wrote on this topic: https://data.library.virginia.edu/getting-started-with-bootstrap-model-validation/

Another type of bootstrap validation is _Calibration_. Calibration is the ability of a model to make unbiased estimates of an outcome. The idea is we would like our model to make accurate predictions for a new set of data not used to develop the model. If we don't have new data, we can use bootstrap resampling. Again the `rms` package makes this relatively easy to carry out with the `calibrate` function. Hopefully the solid "Bias-corrected" line will be diagonal with an intercept of 0 and a slope of 1. That would indicate predicted probabilities are corresponding to actual probabilities. What we see below is that from 0.2 - 0.8 our model severely _under-estimates_ the actual probability. For example, when our model returns a predicted probability of 0.5 (on the x-axis), the actual probability is about 0.6. The reliability of this model for future data may be in doubt.

```{r}
# run entire chunk at once
cal <- calibrate(m_rms, B = 200)
plot(cal)
grid()
```


